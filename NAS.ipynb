{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from graphviz import Digraph\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    def __init__(self, expr = None, msg = None):\n",
    "        self.expr = expr\n",
    "        self.msg = msg\n",
    "class inputSmallerThanKernel(Error):\n",
    "    def __init__(self):\n",
    "        super(inputSmallerThanKernel, self).__init__()\n",
    "class nodeDoesNotExist(Error):\n",
    "    def __init__(self):\n",
    "        super(nodeDoesNotExist, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node(object):\n",
    "    nodes = []\n",
    "    def __init__(self, input_shape = (0, 0, 0), output_shape = (0, 0, 0)): # c, i1, i2\n",
    "        node.nodes.append(self)\n",
    "        self.no  = len(node.nodes)\n",
    "        self.in_adj = []\n",
    "        self.out_adj = []\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.compatible = True\n",
    "            \n",
    "    def node_alright(self, curr_node):\n",
    "        try:\n",
    "            assert(issubclass(type(curr_node), node))\n",
    "        except:\n",
    "            raise Error('Not a node')\n",
    "# Put this section in graph class\n",
    "#         try:\n",
    "#             assert(curr_node in graph_nodes)\n",
    "#         except:\n",
    "#             raise nodeDoesNotExist\n",
    "    \n",
    "    def determine_compatibility(self):\n",
    "        for curr_node in self.in_adj:\n",
    "            curr = (curr_node.output_shape == self.input_shape)\n",
    "            self.compatible  = self.compatible and curr\n",
    "            \n",
    "        for curr_node in self.out_adj:\n",
    "            curr = (curr_node.input_shape == self.output_shape)\n",
    "            self.compatible = self.compatible and curr\n",
    "\n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        assert isinstance(in_adj, list), 'in_adj must be a list'\n",
    "        assert isinstance(in_adj, list), 'out_adj must be a list'\n",
    "        for curr_node in in_adj + out_adj:\n",
    "            try:\n",
    "                self.node_alright(curr_node)\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "        self.in_adj = in_adj\n",
    "        self.out_adj = out_adj\n",
    "\n",
    "    def out_shape(self):\n",
    "        pass\n",
    "    \n",
    "    def remove(self):\n",
    "        ## needed in graph class\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(type(self)) + \" \" + str(self.no)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolution_block(nn.Module, node):\n",
    "    all_convs = []\n",
    "    \n",
    "    def __init__(self, in_h, in_w, in_channels, out_channels, kernel_size, padding = 0, stride = 1):\n",
    "        try:\n",
    "            assert(min(in_h, in_w) +2*padding >= kernel_size)\n",
    "        except:\n",
    "            raise inputSmallerThanKernel\n",
    "        super(convolution_block, self).__init__()\n",
    "        node.__init__(self, (in_channels, in_h, in_w))\n",
    "        convolution_block.all_convs.append(self)\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride \n",
    "        self.padding = padding\n",
    "        self.output_shape = self.out_shape()\n",
    "        \n",
    "        # NN Layers\n",
    "        self.conv_layer = nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding)\n",
    "        self.batch_norm = nn.BatchNorm2d(self.out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    def out_shape(self):\n",
    "        c, h, w = self.input_shape\n",
    "        C = self.out_channels\n",
    "        H = (h + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        W = (w + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        return (C, H, W)\n",
    "    \n",
    "#     def determine_compatibility(self):\n",
    "#         super(convolution_block, self).determine_compatibility()\n",
    "#         self.compatible  = self.compatible and (len(self.in_adj) == 1)\n",
    "\n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        super(convolution_block, self).describe_adj_list(in_adj, out_adj)\n",
    "        try:\n",
    "            assert(len(in_adj) == 1)\n",
    "        except:\n",
    "            print(in_adj)\n",
    "            raise Error('A convolution block can have only one in-edge')\n",
    "    \n",
    "    def remove(self): \n",
    "        ### Remove from all_convs list\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pool_node(nn.Module, node):\n",
    "    all_max_pools = []\n",
    "    \n",
    "    def __init__(self, in_h, in_w, in_channels, kernel_size, padding = 0, stride = 1):\n",
    "        try:\n",
    "            assert(min(in_h, in_w) +2*padding > kernel_size)\n",
    "        except:\n",
    "            raise inputSmallerThanKernel\n",
    "        super(max_pool_node, self).__init__()    \n",
    "        node.__init__(self, (in_channels, in_h, in_w))\n",
    "        max_pool_node.all_max_pools.append(self)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride \n",
    "        self.padding = padding\n",
    "        self.output_shape = self.out_shape()\n",
    "        \n",
    "        ## NN Layer\n",
    "        self.max_pool_layer = nn.MaxPool2d(self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.max_pool_layer(x)\n",
    "    \n",
    "    def out_shape(self):\n",
    "        c, h, w = self.input_shape\n",
    "        C = c\n",
    "        H = (h + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        W = (w + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        return (C, H, W)\n",
    "    \n",
    "#     def determine_compatibility(self):\n",
    "#         super(max_pool_node, self).determine_compatibility()\n",
    "#         self.compatible  = self.compatible and (len(self.in_adj) == 1)\n",
    "\n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        super(max_pool_node, self).describe_adj_list(in_adj, out_adj)\n",
    "        try:\n",
    "            assert(len(in_adj) == 1)\n",
    "        except:\n",
    "            raise Error('A max-pool block can have only one in-edge')\n",
    "\n",
    "    def remove(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Is is it required to be to a derived class of nn.Module ?\n",
    "class merge_node(node):\n",
    "    all_merge_nodes = []\n",
    "    \n",
    "    def __init__(self, parents, child):\n",
    "#         super(merge_node, self).__init__()\n",
    "        node.__init__(self)\n",
    "        try:\n",
    "            self.describe_adj_list([parents[0], parents[1]], [child])\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        merge_node.all_merge_nodes.append(self)\n",
    "        \n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        super(merge_node, self).describe_adj_list(in_adj, out_adj)\n",
    "        try:\n",
    "            assert(len(in_adj) == 2)\n",
    "        except:\n",
    "            raise Error('Parents must be exactly two')\n",
    "        \n",
    "class add_node(nn.Module, merge_node):\n",
    "    all_add_nodes = []\n",
    "    \n",
    "    def __init__(self, parents, child):\n",
    "        super(add_node, self).__init__()\n",
    "        merge_node.__init__(self, parents, child)\n",
    "        add_node.all_add_nodes.append(self)\n",
    "        self.input_shape = self.in_adj[0].output_shape\n",
    "        self.output_shape = self.out_shape()\n",
    "    \n",
    "    ### Does it allow to input paramteters ?\n",
    "    def forward(self, x, y):\n",
    "        return x+y ### Check if their data strcutre type supports this addition\n",
    "        \n",
    "    def out_shape(self):\n",
    "        return self.input_shape\n",
    "    \n",
    "class concat_node(merge_node):\n",
    "    # Define Later\n",
    "    pass\n",
    "\n",
    "class convex_merge_node(merge_node):\n",
    "    # Define Later\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self):\n",
    "        self.adj_mat = {}\n",
    "        self.adj_list = {}\n",
    "        self.nodes = set()\n",
    "        self.int_to_node = {}\n",
    "        self.node_to_int = {}\n",
    "        self.conv_blocks = []\n",
    "        self.max_pool_blocks = [] # Change naming conv maybe ?\n",
    "        self.topsort = []\n",
    "        self.max_no = 0\n",
    "        \n",
    "    def __init__(self, adj_list, int_to_node):\n",
    "        assert isinstance(int_to_node, dict), 'int_to_node must be a dictionary'\n",
    "        for _, cnode in int_to_node.items():\n",
    "            assert isinstance(cnode, node), 'mapping in int_to_node should be to a node'\n",
    "        \n",
    "        assert isinstance(adj_list, dict), 'adj_list should be a dictionary'\n",
    "        assert(len(int_to_node) == len(adj_list))\n",
    "        for cnode, li in adj_list.items():\n",
    "            assert cnode in int_to_node, 'mismatch between int_to_node and adj_list'\n",
    "            try:\n",
    "                assert(isinstance(li, list))\n",
    "                assert(len(li) == 2)\n",
    "                assert(isinstance(li[0], list) and isinstance(li[1], list))\n",
    "            except:\n",
    "                raise Error('Each mapping in adj_list should be to a two-dim list')\n",
    "            for child_node in li[0]:\n",
    "                assert child_node in int_to_node, 'mismatch between int_to_node and adj_list'\n",
    "            for child_node in li[1]:\n",
    "                assert child_node in int_to_node, 'mismatch between int_to_node and adj_list'\n",
    "\n",
    "        self.adj_list = adj_list\n",
    "        self.adj_mat = self.get_adj_mat(self.adj_list)\n",
    "        self.nodes = set(int_to_node.keys())\n",
    "        self.int_to_node = int_to_node\n",
    "        self.node_to_int = self.get_node_to_int(self.int_to_node)\n",
    "        self.max_no = max(self.int_to_node)\n",
    "        self.conv_blocks, self.max_pool_blocks = self.get_conv_and_max_pool_blocks()\n",
    "        self.topsort = self.topsorting()\n",
    "        \n",
    "#     def __init__(self, random_init):\n",
    "#         if random_init:\n",
    "#             # Do random network construction\n",
    "#             pass\n",
    "#         else:\n",
    "#             self.__init__()\n",
    "    \n",
    "    def get_node_to_int(self, int_to_node):\n",
    "        node_to_int = {}\n",
    "        for no, cnode in int_to_node.items():\n",
    "            node_to_int[cnode] = no\n",
    "        return node_to_int\n",
    "\n",
    "        \n",
    "    def get_adj_mat(self, adj_list):\n",
    "        adj_mat = {}\n",
    "        nodes = adj_list.keys()\n",
    "        for x in nodes:\n",
    "            adj_mat[x] = {}\n",
    "            for y in nodes:\n",
    "                adj_mat[x][y] = 0\n",
    "        for cnode, li in adj_list.items():\n",
    "            for par in li[0]:\n",
    "                adj_mat[par][cnode] = 1\n",
    "            for child in li[1]:\n",
    "                adj_mat[cnode][child] = 1\n",
    "        return adj_mat\n",
    "    \n",
    "    def get_conv_and_max_pool_blocks(self):\n",
    "        conv_blocks = []\n",
    "        max_pool_blocks = []\n",
    "        for x in self.nodes:\n",
    "            if isinstance(self.int_to_node[x], convolution_block):\n",
    "                conv_blocks.append(x)\n",
    "            elif isinstance(self.int_to_node[x], max_pool_node):\n",
    "                max_pool_blocks.append(x)\n",
    "        return (conv_blocks, max_pool_blocks)\n",
    "    \n",
    "    def topsorting(self):\n",
    "        # level problem\n",
    "        topsort = []\n",
    "        import Queue\n",
    "        in_deg = {}\n",
    "        q = Queue.Queue()\n",
    "        for node in self.nodes:\n",
    "            val  = len(self.adj_list[node][0])\n",
    "#             val = len(self.int_to_node[node].in_adj)\n",
    "            if val == 0:\n",
    "                q.put(node)\n",
    "            in_deg[node] = val\n",
    "            \n",
    "        while not q.empty():\n",
    "            curr_node = q.get()\n",
    "            topsort.append(curr_node)\n",
    "            for child in self.adj_list[curr_node][1]:\n",
    "                in_deg[child] -= 1\n",
    "                if in_deg[child] == 0:\n",
    "                    q.put(child)\n",
    "        return topsort\n",
    "    \n",
    "    def add_nodes_to_network(self, nodes):\n",
    "        ### loophole here, assumption is that all changed nodes are being provided to the function\n",
    "        ### for now, lets go on with it, but its an issue\n",
    "        for curr_node in nodes:\n",
    "            curr_node.determine_compatibility()\n",
    "            if not curr_node.compatible:\n",
    "                raise Error('Node is not compatible with the graph') \n",
    "        for curr_node in nodes:\n",
    "            if curr_node not in self.node_to_int:\n",
    "                self.max_no += 1\n",
    "                self.adj_mat[self.max_no] = {}\n",
    "                self.adj_list[self.max_no] = []\n",
    "                self.node_to_int[curr_node] = self.max_no\n",
    "                self.int_to_node[self.max_no] = curr_node\n",
    "                self.nodes.add(self.max_no)\n",
    "                if isinstance(curr_node, convolution_block):\n",
    "                    self.conv_blocks.append(self.max_no)\n",
    "                elif isinstance(curr_node, max_pool_node):\n",
    "                    self.max_pool_blocks.append(self.max_no)\n",
    "        for curr_node in nodes:\n",
    "            no = self.node_to_int[curr_node]\n",
    "            self.adj_list[no] = [map(lambda x: self.node_to_int[x], curr_node.in_adj), map(lambda x: self.node_to_int[x], curr_node.out_adj)]\n",
    "            for par in self.adj_list[no][0]:\n",
    "                self.adj_mat[par][no] = 1\n",
    "            for child in self.adj_list[no][0]:\n",
    "                self.adj_mat[no][child] = 1\n",
    "        self.topsort = self.topsorting()\n",
    "        \n",
    "    def morphism(self):\n",
    "        import random\n",
    "        actions = {'deepen': self.deepen_morph, \n",
    "                   'widen': self.widen_morph, \n",
    "                   'skip-connection': self.skip_morph }\n",
    "        choice = random.choice(actions)\n",
    "        actions[choice]()\n",
    "    \n",
    "    def deepen_morph(self):\n",
    "        deepen_conv_block = self.int_to_node[random.choice(self.conv_blocks)]\n",
    "        kernel_size = random.choice([3, 5])\n",
    "        in_channels, in_h, in_w = deepen_conv_block.output_shape\n",
    "        out_channels = in_channels\n",
    "        identity_conv_block = convolution_block(in_h, in_w, in_channels, out_channels, kernel_size, (kernel_size-1)/2)\n",
    "        weights = identity_conv_block.conv_layer.weight.data\n",
    "        \n",
    "        # creating identity weights\n",
    "        for channel in range(out_channels):\n",
    "            for i in range(in_channels):\n",
    "                for j in range(kernel_size):\n",
    "                    for k in range(kernel_size):\n",
    "                        weights[channel][i][j][k] = int((channel == i) and (j == k) and j == (kernel_size)/2 )\n",
    "#         print 'weights of identity conv block', weights\n",
    "        \n",
    "        ## make connections \n",
    "        identity_conv_block.describe_adj_list([deepen_conv_block], deepen_conv_block.out_adj)\n",
    "        deepen_conv_block.describe_adj_list(deepen_conv_block.in_adj, [identity_conv_block])\n",
    "\n",
    "        #### later look at creating a function for singular change to in_adj or out_adj of nodes\n",
    "        for out_node in identity_conv_block.out_adj:\n",
    "            out_node_in_adj = [identity_conv_block if (x == deepen_conv_block) else x for x in out_node.in_adj ]\n",
    "            out_node.describe_adj_list(out_node_in_adj, out_node.out_adj)\n",
    "        \n",
    "        self.add_nodes_to_network([deepen_conv_block, identity_conv_block] + identity_conv_block.out_adj)\n",
    "    \n",
    "    \n",
    "    def widen_morph(self):\n",
    "        candidate_conv_blocks = []\n",
    "        for conv_block in self.conv_blocks:\n",
    "            isCandidate = bool(len(self.adj_list[conv_block][1]))\n",
    "            for child in self.adj_list[conv_block][1]:\n",
    "                isCandidate = isCandidate and isinstance(self.int_to_node[child], convolution_block)\n",
    "            if isCandidate:\n",
    "                candidate_conv_blocks.append(conv_block)\n",
    "        if len(candidate_conv_blocks) == 0:\n",
    "            return False\n",
    "\n",
    "        parent_block_no = random.choice(candidate_conv_blocks)\n",
    "        print candidate_conv_blocks\n",
    "        print parent_block_no\n",
    "        parent_block = self.int_to_node[parent_block_no]\n",
    "        widening_factor = random.choice([2, 4])\n",
    "        in_channels, in_h, in_w = parent_block.input_shape\n",
    "        out_channels = parent_block.out_channels\n",
    "        kernel_size = parent_block.kernel_size\n",
    "        padding = parent_block.padding\n",
    "        stride = parent_block.stride\n",
    "        widened_parent_block = convolution_block(in_h, in_w, in_channels, out_channels*widening_factor, kernel_size, padding, stride)\n",
    "        original_parent_weight = parent_block.conv_layer.weight.data\n",
    "        widened_parent_weight = widened_parent_block.conv_layer.weight.data\n",
    "        widened_parent_weight[:out_channels] = original_parent_weight\n",
    "        widened_parent_weight[out_channels:] = torch.zeros((out_channels*(widening_factor-1), in_channels, kernel_size, kernel_size))\n",
    "        self.int_to_node[parent_block_no]  = widened_parent_block\n",
    "        del self.node_to_int[parent_block]\n",
    "        self.node_to_int[widened_parent_block] = parent_block_no\n",
    "        parent_out_adj = []\n",
    "        for child in parent_block.out_adj:\n",
    "            child_no = self.node_to_int[child]\n",
    "            in_channels, in_h, in_w = child.input_shape\n",
    "            out_channels = child.out_channels\n",
    "            kernel_size = child.kernel_size \n",
    "            padding = child.padding\n",
    "            stride = child.stride\n",
    "            child_widened = convolution_block(in_h, in_w, in_channels*widening_factor, out_channels, kernel_size, padding, stride)\n",
    "            child_widened.conv_layer.weight.data[:, :in_channels, :, :] = child.conv_layer.weight.data\n",
    "            child_widened.describe_adj_list([widened_parent_block if x == parent_block else x in child.in_adj], child.out_adj)\n",
    "            self.int_to_node[child_no] = child_widened\n",
    "            del self.node_to_int[child]\n",
    "            self.node_to_int[child_widened] = child_no\n",
    "            parent_out_adj.append(widened_child)\n",
    "        widened_parent_block.describe_adj_list(parent_block.in_adj, parent_out_adj)\n",
    "        \n",
    "    def skip_morph(self):\n",
    "        pass\n",
    "    \n",
    "    def visualize(self):\n",
    "        graph = Digraph('arch', 'arch.gv')\n",
    "        for no, curr_node in self.int_to_node.items():\n",
    "#             graph.node(str(no), str(type(curr_node)).split('__main__.')[1])\n",
    "            graph.node(str(no), str(self.node_to_int[curr_node]) + \" :: \" + repr(curr_node)[:200])\n",
    "        for no, li in self.adj_list.items():\n",
    "            for ch in li[1]:\n",
    "                graph.edge(str(no), str(ch))\n",
    "        graph.view()\n",
    "    \n",
    "    def describe(self):\n",
    "        print 'Nodes: ', self.nodes\n",
    "        print 'Conv_blocks', self.conv_blocks\n",
    "        print 'Max_pool_blocks', self.max_pool_blocks\n",
    "        print 'Adj_list', self.adj_list\n",
    "        print 'Adj_mat', self.adj_mat\n",
    "        print 'int_to_node', self.int_to_node\n",
    "        print 'node_to_int', self.node_to_int\n",
    "        print 'Toposort', self.topsort\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = node((5, 5, 5), (5, 5, 5))\n",
    "n1 = convolution_block(5, 5, 5, 4, 3)\n",
    "n2 = convolution_block(3, 3, 4, 3, 2)\n",
    "dummy.describe_adj_list([], [n1])\n",
    "n1.describe_adj_list([dummy], [n2])\n",
    "n2.describe_adj_list([n1], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network({0:[[], [1]], 1:[[0], [2]], 2: [[1], []]}, {0: dummy, 1:n1, 2:n2})\n",
    "#### remove morph function from Network class to shift to hill climbing, also kernel_size and widening factor \n",
    "#### for deepen and widen should be parameters\n",
    "### check more conformity betweeen input adj_list and input 'int_to_node'\n",
    "### check compatibility of initial arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:  set([0, 1, 2])\n",
      "Conv_blocks [1, 2]\n",
      "Max_pool_blocks []\n",
      "Adj_list {0: [[], [1]], 1: [[0], [2]], 2: [[1], []]}\n",
      "Adj_mat {0: {0: 0, 1: 1, 2: 0}, 1: {0: 0, 1: 0, 2: 1}, 2: {0: 0, 1: 0, 2: 0}}\n",
      "int_to_node {0: <class '__main__.node'> 21, 1: convolution_block(\n",
      "  (conv_layer): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batch_norm): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "), 2: convolution_block(\n",
      "  (conv_layer): Conv2d(4, 3, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (batch_norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      ")}\n",
      "node_to_int {convolution_block(\n",
      "  (conv_layer): Conv2d(4, 3, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (batch_norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "): 2, <class '__main__.node'> 21: 0, convolution_block(\n",
      "  (conv_layer): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batch_norm): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "): 1}\n",
      "Toposort [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "net.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 5)  ->  0 -> (5, 5, 5)\n",
      "(5, 5, 5)  ->  1 -> (4, 3, 3)\n",
      "(4, 3, 3)  ->  2 -> (3, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "net.visualize()\n",
    "for nan in net.nodes:\n",
    "    print net.int_to_node[nan].input_shape, ' -> ', nan, '->', net.int_to_node[nan].output_shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-d706c70f305d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwiden_morph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-42d224152539>\u001b[0m in \u001b[0;36mwiden_morph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mchild_widened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolution_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwidening_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mchild_widened\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mchild_widened\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_adj_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwidened_parent_block\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mparent_block\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_adj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_to_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild_no\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchild_widened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "net.widen_morph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:  set([0, 1, 2])\n",
      "Conv_blocks [1, 2]\n",
      "Max_pool_blocks []\n",
      "Adj_list {0: [[], [1]], 1: [[0], [2]], 2: [[1], []]}\n",
      "Adj_mat {0: {0: 0, 1: 1, 2: 0}, 1: {0: 0, 1: 0, 2: 1}, 2: {0: 0, 1: 0, 2: 0}}\n",
      "int_to_node {0: <class '__main__.node'> 10, 1: convolution_block(\n",
      "  (conv_layer): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batch_norm): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "), 2: convolution_block(\n",
      "  (conv_layer): Conv2d(4, 12, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (batch_norm): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      ")}\n",
      "node_to_int {<class '__main__.node'> 10: 0, convolution_block(\n",
      "  (conv_layer): Conv2d(4, 12, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (batch_norm): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "): 2, convolution_block(\n",
      "  (conv_layer): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batch_norm): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "): 1}\n",
      "Toposort [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "net.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 5)  ->  0 -> (5, 5, 5)\n",
      "(5, 5, 5)  ->  1 -> (4, 3, 3)\n",
      "(4, 3, 3)  ->  2 -> (12, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "net.visualize()\n",
    "for nan in net.nodes:\n",
    "    print net.int_to_node[nan].input_shape, ' -> ', nan, '->', net.int_to_node[nan].output_shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "www = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "www.emp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
